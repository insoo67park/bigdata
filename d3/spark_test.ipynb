{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RDD data 1', 'RDD data 2']\n",
      "RDD data 1\n",
      "['82, 58, 49, 99, 25, 37, 44, 94, 70, 18, 73, 81, 32, 21, 79, 13, 38, 51, 70, 38, 82, 43, 42', '63, 78, 39, 1, 56, 35, 93, 66, 5, 91, 77, 86, 45, 58, 77, 49, 47, 15, 92, 12', '70, 38, 49, 58, 96, 79, 69, 94, 50, 11, 86, 39, 58, 74, 69, 82, 97', '84, 92, 92, 80, 98, 44, 27, 55, 52, 47, 44, 96, 64, 37, 30, 18, 18, 49, 53, 43', '84, 68, 38, 78, 23, 16, 41, 55, 50, 89, 9, 78, 23, 95, 24, 29, 21, 4, 71']\n",
      "['82, 58, 49, 99, 25, 37, 44, 94, 70, 18, 73, 81, 32, 21, 79, 13, 38, 51, 70, 38, 82, 43, 42', '63, 78, 39, 1, 56, 35, 93, 66, 5, 91, 77, 86, 45, 58, 77, 49, 47, 15, 92, 12', '70, 38, 49, 58, 96, 79, 69, 94, 50, 11, 86, 39, 58, 74, 69, 82, 97', '84, 92, 92, 80, 98, 44, 27, 55, 52, 47, 44, 96, 64, 37, 30, 18, 18, 49, 53, 43', '84, 68, 38, 78, 23, 16, 41, 55, 50, 89, 9, 78, 23, 95, 24, 29, 21, 4, 71']\n"
     ]
    }
   ],
   "source": [
    "str_data = ['RDD data 1', 'RDD data 2']\n",
    "#rdd_data = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "rdd_data = sc.parallelize(str_data)\n",
    "\n",
    "data_from_collect = rdd_data.collect()\n",
    "data_from_first = rdd_data.first()\n",
    "\n",
    "print(data_from_collect)\n",
    "print(data_from_first)\n",
    "\n",
    "# 읽어온 데이터를 다른 경로에 저장\n",
    "data_in_hdfs = sc.textFile(\"/sorting_input/sorting.txt\")\n",
    "print(data_in_hdfs.collect())\n",
    "\n",
    "# 저장된 데이터 살펴보기\n",
    "data_in_hdfs.saveAsTextFile(\"/tmp/insoo67_park/sorting_input\")\n",
    "data_in_hdfs_read = sc.textFile(\"/tmp/insoo67_park/sorting_input/*\")\n",
    "print(data_in_hdfs_read.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_files MapPartitionsRDD[30] at textFile at NativeMethodAccessorImpl.java:0\n"
     ]
    }
   ],
   "source": [
    "no_files_RDD = sc.textFile(\"no_files\") # RDD 연결만 만듦\n",
    "print(no_files_RDD) # error 발생 안 함\n",
    "\n",
    "#no_files_RDD.collect() # 연산 수행, error 발생\n",
    "#no_files_RDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'M_store': 2, 'S_cafe': 2})\n",
      "defaultdict(<class 'int'>, {'S_cafe': 2, 'M_store': 1})\n"
     ]
    }
   ],
   "source": [
    "#Exercise 1-1\n",
    "A = sc.parallelize(['M_store', 'S_cafe', 'M_store', 'S_cafe', 'H_shop', 'D_store'])\n",
    "B = sc.parallelize(['S_cafe', 'M_store', 'S_cafe', 'A_market', 'A_market'])\n",
    "\n",
    "A_B_iter = A.intersection(B).collect()\n",
    "\n",
    "def f(x):\n",
    "    return x in A_B_iter\n",
    "\n",
    "A_visit = A.filter(f)\n",
    "B_visit = B.filter(f)\n",
    "\n",
    "A_visit_countByValue = A_visit.countByValue()\n",
    "B_visit_countByValue = B_visit.countByValue()\n",
    "\n",
    "print(A_visit_countByValue)\n",
    "print(B_visit_countByValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no_signal', 'no_material']\n"
     ]
    }
   ],
   "source": [
    "#Exercise 1-2\n",
    "all_data = sc.parallelize(['success', 'success', 'error:no_signal', 'success', 'error:no_material'])\n",
    "\n",
    "error_list = all_data.filter(lambda x : x[0] == \"e\")\n",
    "error_reason = error_list.map(lambda s: s.split(\":\")[1])\n",
    "\n",
    "print(error_reason.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173 Mil.\n"
     ]
    }
   ],
   "source": [
    "#Exercise 1-3\n",
    "all_data = sc.parallelize([\"Samsung:72\", \"Huawei:59\", \"Apple:42\"])\n",
    "shipment = all_data.map(lambda s: int(s.split(\":\")[1]))\n",
    "sum_shipment = shipment.reduce(lambda a, b: a+b)\n",
    "print(sum_shipment, \"Mil.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.566666666666666 Mil.\n"
     ]
    }
   ],
   "source": [
    "#Exercise 1-4\n",
    "all_data = sc.parallelize([\"Galaxy,A10,30.3\", \"Galaxy,A50,24.2\", \"Galaxy,A20,19.2\", \"iPhone,XR,46.3\", \"iPhone,11,37.3\", \"Redmi,Note7,16.4\"])\n",
    "\n",
    "Galaxy_data = all_data.filter(lambda x: x.split(\",\")[0] == \"Galaxy\")\n",
    "Galaxy_shipment = Galaxy_data.map(lambda s: float(s.split(\",\")[2]))\n",
    "\n",
    "Galaxy_shipment_mean = Galaxy_shipment.mean()\n",
    "print(Galaxy_shipment_mean, 'Mil.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Note20', 26), ('a21s', 48), ('s20', 35)]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2-1\n",
    "from operator import add\n",
    "RDD = sc.parallelize([(\"Note20\", 30), (\"Note20\", -4), (\"s20\", -12), (\"s20\", 20), (\"s20\", -2), (\"s20\", -3), (\"s20\", 32), (\"a21s\", -19), (\"a21s\", 67)])\n",
    "\n",
    "#print(RDD.groupByKey().mapValues(list).collect())\n",
    "result = RDD.reduceByKey(add).sortByKey().collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('CS', [' Seoul', ' Suwon', ' Busan', ' Daejeon']), ('R&D', [' seoul', ' Daejeon'])]\n",
      "[('CS', 4), ('R&D', 2)]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2-2\n",
    "RDD = sc.parallelize([\"CS, Seoul\", \"CS, Suwon\", \"CS, Busan\", \"CS, Daejeon\", \"R&D, seoul\", \"R&D, Daejeon\"])\n",
    "\n",
    "def f(x):\n",
    "    xx = x.split(\",\")\n",
    "    return (xx[0], xx[1])\n",
    "\n",
    "group_by_key_list = RDD.map(f).groupByKey().mapValues(list).collect()\n",
    "group_by_key_count = RDD.map(f).groupByKey().mapValues(len).collect()\n",
    "\n",
    "print(group_by_key_list)\n",
    "print(group_by_key_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Thriller', 50), ('Billie_Jean', 19)]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2-3\n",
    "A = sc.parallelize([('Thriller', 30), ('Everybody', 34), ('Everybody', 30), ('Billie_Jean', 12), ('Billie_Jean', 2)])\n",
    "B = sc.parallelize([('Thriller', 20), ('Sorry', 23), ('Sorry', 3), ('Billie_Jean', 5)])\n",
    "\n",
    "# A_groupByKey = A.groupByKey().mapValues(list)\n",
    "# B_groupByKey = B.groupByKey().mapValues(list))\n",
    "\n",
    "# A_B_iter = A_groupByKey.intersection(B_groupByKey).collect()\n",
    "# print(A_B_iter)\n",
    "\n",
    "# def f(x):\n",
    "#    return x in A_B_iter\n",
    "\n",
    "# A_saw = A_groupByKey.filter(f)\n",
    "# B_saw = B_groupByKey.filter(f)\n",
    "\n",
    "# A_sawt_countByValue = A_saw.countByValue()\n",
    "# B_saw_countByValue = B_saw.countByValue()\n",
    "\n",
    "# print(A_sawt_countByValue)\n",
    "# print(B_saw_countByValue)\n",
    "from operator import add\n",
    "A_reduceByKey = A.reduceByKey(add)\n",
    "B_reduceByKey = B.reduceByKey(add)\n",
    "\n",
    "AB_join = A_reduceByKey.join(B_reduceByKey)\n",
    "\n",
    "def f(x):\n",
    "    return sum(x)\n",
    "\n",
    "AB_join_mapValues = AB_join.mapValues(f).collect()\n",
    "print(AB_join_mapValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|model|count|\n",
      "+-----+-----+\n",
      "| flex|    7|\n",
      "|  ion|    4|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3-1\n",
    "from pyspark.sql import Row\n",
    "DF1 = sc.parallelize(\n",
    "[Row(phase='packing', model='flex', serial='flex1'),\n",
    "Row(phase='packing',  model='flex', serial='flex2'),\n",
    "Row(phase='packing',  model='ion', serial='ion1'),\n",
    "Row(phase='packing',  model='flex', serial='flex3'),\n",
    "Row(phase='packing',  model='ion', serial='ion2'),\n",
    "Row(phase='inspection',  model='flex', serial='flex4'),\n",
    "Row(phase='inspection',  model='ion', serial='ion3'),\n",
    "Row(phase='inspection',  model='flex', serial='flex5'),\n",
    "Row(phase='inspection',  model='flex', serial='flex6'),\n",
    "]).toDF()\n",
    "\n",
    "DF2 = sc.parallelize(\n",
    "[\n",
    "    Row(phase='assembly', model='flex', serial='flex7'),\n",
    "    Row(phase='assembly',  model='ion', serial='ion4')\n",
    "]\n",
    ").toDF()\n",
    "DF1.unionByName(DF2).select(\"model\", \"phase\").groupBy(\"model\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Note20', 43), ('s21', 48), ('a21s', 99)]\n"
     ]
    }
   ],
   "source": [
    "# Homework\n",
    "A_store = sc.parallelize([(\"Note20\", 30), (\"Note20\", -4), (\"s21\", -12), (\"s21\", 20), (\"s21\", -2), (\"s21\", -3), (\"s21\", 32), (\"a21s\", -19), (\"a21s\", 67)])\n",
    "B_store = sc.parallelize([(\"Note20\", 20), (\"Note20\", -3), (\"s21\", -10), (\"s21\", 20), (\"s21\", -1), (\"s21\", -6), (\"s21\", 10), (\"a21s\", -9), (\"a21s\", 60)])\n",
    "\n",
    "from operator import add\n",
    "A_reduceByKey = A_store.reduceByKey(add)\n",
    "B_reduceByKey = B_store.reduceByKey(add)\n",
    "\n",
    "AB_join = A_reduceByKey.join(B_reduceByKey)\n",
    "\n",
    "def f(x):\n",
    "    return sum(x)\n",
    "\n",
    "AB_join_mapValues = AB_join.mapValues(f).collect()\n",
    "print(AB_join_mapValues)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
